{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-17T16:02:00.286646Z",
     "iopub.status.busy": "2025-09-17T16:02:00.285920Z",
     "iopub.status.idle": "2025-09-17T16:03:53.810879Z",
     "shell.execute_reply": "2025-09-17T16:03:53.810158Z",
     "shell.execute_reply.started": "2025-09-17T16:02:00.286617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.21.3 datasets==2.12.0 torchaudio soundfile evaluate jiwer scikit-learn matplotlib seaborn tensorboard torch-audiomentations\n",
    "!pip install -q peft==0.4.0  # Compatible version with transformers 4.21.3\n",
    "!pip install editdistance\n",
    "!pip install -q accelerate  # Required for training  \n",
    "# install evaluate and jiwer quietly\n",
    "!pip install -q evaluate jiwer           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-17T16:05:39.816Z",
     "iopub.execute_input": "2025-09-17T16:05:38.206618Z",
     "iopub.status.busy": "2025-09-17T16:05:38.206076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Restart runtime (this will kill the current process and Kaggle will restart the kernel)\n",
    "import os, signal\n",
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:05:54.288166Z",
     "iopub.status.busy": "2025-09-17T16:05:54.287884Z",
     "iopub.status.idle": "2025-09-18T01:32:31.754325Z",
     "shell.execute_reply": "2025-09-18T01:32:31.753413Z",
     "shell.execute_reply.started": "2025-09-17T16:05:54.288144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np \n",
    "from collections import defaultdict, Counter\n",
    "import editdistance\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "import time  # Add time import\n",
    "from transformers import TrainerCallback\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Hugging Face imports\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,  \n",
    "    Wav2Vec2ForCTC, \n",
    "    Wav2Vec2Processor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import evaluate\n",
    "\n",
    "# PyTorch and sklearn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED) \n",
    "\n",
    "# Hyperparameters - clearly defined at the top\n",
    "BATCH_SIZE = 28   \n",
    "EPOCHS = 8    \n",
    "LEARNING_RATE = 3e-4  # Conservative for fine-tuning wav2vec2\n",
    "WARMUP_STEPS = 500\n",
    "MAX_AUDIO_SECONDS = 20  # Will be computed dynamically but this is fallback\n",
    "TRAIN_SIZE = 8700\n",
    "VAL_SIZE = 1087\n",
    "TEST_SIZE = 1088\n",
    "\n",
    "# Paths\n",
    "AUDIO_DIR = \"/kaggle/input/aasrnew/Dataset/TRwav\"\n",
    "TRANSCRIPT_CSV = \"/kaggle/input/aasrnew/Dataset/aligned_transcripts.csv\"\n",
    "OUTPUT_DIR = \"/kaggle/working/model\"\n",
    "PLOTS_DIR = \"/kaggle/working/plots\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Amharic phoneme mapping and tokenizer\n",
    "class AmharicPhonemeTokenizer:\n",
    "        \n",
    "    def __init__(self):\n",
    "        # Consonants based on your transliteration scheme\n",
    "        consonants = [\n",
    "            # Basic consonants\n",
    "            'b', 'p', 't', 'd', 'k', 'g', 'q', 'h', 'x', 'f', 'v', 's', 'z', \n",
    "            'sh', 'zh', 'ch', 'j', 'm', 'n', 'ny', 'ng', 'l', 'r', 'y', 'w',\n",
    "            # Ejectives (with apostrophe)\n",
    "            \"p'\", \"t'\", \"k'\", \"q'\", \"ts'\", \"ch'\", \"s'\", \"sh'\",\n",
    "            # Geminated consonants (doubled)\n",
    "            'bb', 'pp', 'tt', 'dd', 'kk', 'gg', 'qq', 'ff', 'ss', 'zz', \n",
    "            'll', 'rr', 'mm', 'nn', 'yy', 'ww',\n",
    "            # Consonant clusters \n",
    "            'br', 'dr', 'gr', 'kr', 'pr', 'tr', 'bl', 'dl', 'gl', 'kl', 'pl', 'tl'\n",
    "        ]\n",
    "        \n",
    "        # Vowels including long forms and uppercase variants from your data\n",
    "        vowels = [\n",
    "            'a', 'e', 'i', 'o', 'u',           # Basic vowels\n",
    "            'aa', 'ee', 'ii', 'oo', 'uu',      # Long vowels\n",
    "            'A', 'E', 'I', 'O', 'U'            # Capital vowels from the dataset\n",
    "        ]\n",
    "        \n",
    "        # Special phonemes and markers from your transliteration\n",
    "        special_phonemes = [\n",
    "            \"'\",     # glottal stop/ejective marker\n",
    "            \"~\",     # nasalization marker ( \"yAneda~nA\")\n",
    "            \"^\",     # stress/tone marker (like \"dara^gA\")\n",
    "            \"ə\",     # schwa/epenthetic vowel\n",
    "            \"ɨ\",     # central vowel\n",
    "            \"ʔ\"      # another glottal representation\n",
    "        ]\n",
    "        \n",
    "        # Build vocabulary with special tokens\n",
    "        self.special_tokens = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\", \"<blank>\", \"<sil>\"]\n",
    "        all_phonemes = self.special_tokens + consonants + vowels + special_phonemes\n",
    "        \n",
    "        # Create token mappings\n",
    "        self.token_to_id = {token: i for i, token in enumerate(all_phonemes)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(all_phonemes)}\n",
    "        self.vocab_size = len(all_phonemes)\n",
    "        \n",
    "        # CTC blank token\n",
    "        self.blank_token_id = self.token_to_id[\"<blank>\"]\n",
    "        self.pad_token_id = self.token_to_id[\"<pad>\"]\n",
    "        self.unk_token_id = self.token_to_id[\"<unk>\"]\n",
    "        \n",
    "        print(f\"Transliterated Amharic vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"Sample phonemes: {list(self.token_to_id.keys())[:20]}\")\n",
    "    \n",
    "    def _text_to_phonemes(self, text: str) -> List[str]:\n",
    "        \n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Handle common transliteration patterns specific to the dataset\n",
    "        phonemes = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(text):\n",
    "            # Skip spaces - convert to word boundaries\n",
    "            if text[i] == ' ':\n",
    "                if phonemes and phonemes[-1] != '<sil>':\n",
    "                    phonemes.append('<sil>')  # Word boundary marker\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Handle punctuation and special characters \n",
    "            if text[i] in '.~^\\'':\n",
    "                if text[i] == '~':\n",
    "                    # Nasalization marker - add as separate token\n",
    "                    phonemes.append('~')\n",
    "                elif text[i] == '^':\n",
    "                    # Stress or tone marker\n",
    "                    phonemes.append('^')\n",
    "                elif text[i] == '\\'':\n",
    "                    # Glottal stop or ejective marker\n",
    "                    phonemes.append(\"'\")\n",
    "                elif text[i] == '.':\n",
    "                    phonemes.append('<sil>')\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Process phoneme sequences - check longer sequences first\n",
    "            found = False\n",
    "            \n",
    "            # Check for 3-character sequences (such as ejectives \"ch'\", \"ts'\")\n",
    "            if i <= len(text) - 3:\n",
    "                three_char = text[i:i+3]\n",
    "                if three_char in self.token_to_id:\n",
    "                    phonemes.append(three_char)\n",
    "                    i += 3\n",
    "                    found = True\n",
    "                elif three_char.endswith(\"'\") and three_char[:-1] in ['ch', 'ts', 'sh']:\n",
    "                    # Handle ejective consonants\n",
    "                    base_consonant = three_char[:-1]\n",
    "                    if base_consonant == 'ch':\n",
    "                        phonemes.append(\"ch'\")\n",
    "                    elif base_consonant == 'ts':\n",
    "                        phonemes.append(\"ts'\")\n",
    "                    elif base_consonant == 'sh':\n",
    "                        phonemes.append(\"sh'\")\n",
    "                    i += 3\n",
    "                    found = True\n",
    "            \n",
    "            # Check for 2-character sequences (such as \"ch\", \"sh\", \"ng\", \"ny\")\n",
    "            if not found and i <= len(text) - 2:\n",
    "                two_char = text[i:i+2]\n",
    "                if two_char in self.token_to_id:\n",
    "                    phonemes.append(two_char)\n",
    "                    i += 2\n",
    "                    found = True\n",
    "                # Handle specific transliteration patterns from the data\n",
    "                elif two_char in ['ch', 'sh', 'zh', 'ng', 'ny', 'ts']:\n",
    "                    phonemes.append(two_char)\n",
    "                    i += 2\n",
    "                    found = True\n",
    "                # Handle gemination (doubled consonants)\n",
    "                elif two_char[0] == two_char[1] and two_char[0] in 'bcdfghjklmnpqrstvwxz':\n",
    "                    # Double consonant - add both for gemination\n",
    "                    phonemes.extend([two_char[0], two_char[0]])\n",
    "                    i += 2\n",
    "                    found = True\n",
    "                # Handle vowel sequences (diphthongs or length)\n",
    "                elif two_char in ['aa', 'ee', 'ii', 'oo', 'uu']:\n",
    "                    # Long vowels\n",
    "                    phonemes.append(two_char)\n",
    "                    i += 2\n",
    "                    found = True\n",
    "                # Check for consonant and apostrophe ejective (such as \"p'\", \"t'\")\n",
    "                elif text[i+1] == \"'\":\n",
    "                    two_char = text[i:i+2]  # e.g., \"p'\"\n",
    "                    if two_char in self.token_to_id:\n",
    "                        phonemes.append(two_char)\n",
    "                        i += 2\n",
    "                        found = True\n",
    "            \n",
    "            # Single character processing\n",
    "            if not found:\n",
    "                char = text[i]\n",
    "                if char in self.token_to_id:\n",
    "                    phonemes.append(char)\n",
    "                elif char.isalpha():\n",
    "                    # Map common transliteration characters to closest phonemes\n",
    "                    char_map = {\n",
    "                        'c': 'ch',  # 'c' often represents 'ch' in Amharic transliteration\n",
    "                        'x': 'ks',  # 'x' sound\n",
    "                        'q': 'k',   # Amharic q sound (fallback)\n",
    "                        'j': 'zh',  # 'j' might represent 'zh' sound\n",
    "                    }\n",
    "                    mapped_char = char_map.get(char, char)\n",
    "                    if mapped_char in self.token_to_id:\n",
    "                        phonemes.append(mapped_char)\n",
    "                    else:\n",
    "                        # Try to handle as unknown but valid phoneme\n",
    "                        phonemes.append(\"<unk>\")\n",
    "                else:\n",
    "                    phonemes.append(\"<unk>\")\n",
    "                i += 1\n",
    "        \n",
    "        # Post-processing: clean up and apply basic phonological rules\n",
    "        cleaned_phonemes = []\n",
    "        for j, phoneme in enumerate(phonemes):\n",
    "            # Remove consecutive silence markers\n",
    "            if phoneme == '<sil>' and cleaned_phonemes and cleaned_phonemes[-1] == '<sil>':\n",
    "                continue\n",
    "            \n",
    "            # Handle nasalization markers\n",
    "            if phoneme == '~' and cleaned_phonemes:\n",
    "                # Modify previous phoneme to indicate nasalization\n",
    "                prev_phoneme = cleaned_phonemes[-1]\n",
    "                if prev_phoneme not in self.special_tokens:\n",
    "                    cleaned_phonemes[-1] = prev_phoneme + 'n'  # Add nasal quality\n",
    "                continue\n",
    "            \n",
    "            cleaned_phonemes.append(phoneme)\n",
    "        \n",
    "        # Remove trailing silence\n",
    "        if cleaned_phonemes and cleaned_phonemes[-1] == '<sil>':\n",
    "            cleaned_phonemes.pop()\n",
    "        \n",
    "        return cleaned_phonemes\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
    "        \"\"\"Encode text to token IDs using phoneme processing\"\"\"\n",
    "        phonemes = self._text_to_phonemes(text)\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            phonemes = [\"<s>\"] + phonemes + [\"</s>\"]\n",
    "        \n",
    "        token_ids = [self.token_to_id.get(p, self.unk_token_id) for p in phonemes]\n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"Decode token IDs back to phoneme text\"\"\"\n",
    "        tokens = [self.id_to_token[t] for t in token_ids if t in self.id_to_token]\n",
    "        \n",
    "        if skip_special_tokens:\n",
    "            tokens = [t for t in tokens if t not in self.special_tokens]\n",
    "        \n",
    "        # Join with spaces, but handle silence tokens appropriately\n",
    "        result = []\n",
    "        for token in tokens:\n",
    "            if token == '<sil>':\n",
    "                result.append(' ')\n",
    "            else:\n",
    "                result.append(token)\n",
    "        \n",
    "        return ' '.join(result).replace('  ', ' ').strip()\n",
    "    \n",
    "    def save_pretrained(self, save_directory: str):\n",
    "        \"\"\"Save tokenizer\"\"\"\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "        \n",
    "        vocab_file = os.path.join(save_directory, \"vocab.json\")\n",
    "        with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.token_to_id, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        config_file = os.path.join(save_directory, \"tokenizer_config.json\")\n",
    "        config = {\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"blank_token_id\": self.blank_token_id,\n",
    "            \"pad_token_id\": self.pad_token_id,\n",
    "            \"unk_token_id\": self.unk_token_id\n",
    "        }\n",
    "        with open(config_file, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: str):\n",
    "        \"\"\"Load tokenizer\"\"\"\n",
    "        tokenizer = cls()\n",
    "        \n",
    "        vocab_file = os.path.join(save_directory, \"vocab.json\")\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "            tokenizer.token_to_id = json.load(f)\n",
    "        \n",
    "        tokenizer.id_to_token = {v: k for k, v in tokenizer.token_to_id.items()}\n",
    "        tokenizer.vocab_size = len(tokenizer.token_to_id)\n",
    "        \n",
    "        config_file = os.path.join(save_directory, \"tokenizer_config.json\")\n",
    "        with open(config_file, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        tokenizer.blank_token_id = config[\"blank_token_id\"]\n",
    "        tokenizer.pad_token_id = config[\"pad_token_id\"] \n",
    "        tokenizer.unk_token_id = config[\"unk_token_id\"]\n",
    "        \n",
    "        return tokenizer\n",
    "\n",
    "# Test with the actual dataset format\n",
    "tokenizer = AmharicPhonemeTokenizer()\n",
    "test_text = \"yAneda~nA dara^gA temeheretA^cawene gonedare tamerawAle\"\n",
    "phonemes = tokenizer.encode(test_text)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Phonemes: {tokenizer.decode(phonemes)}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "phoneme_tokenizer = AmharicPhonemeTokenizer()\n",
    "\n",
    "# Data loading and verification\n",
    "def find_transcript_file(base_path: str) -> str:\n",
    "    \"\"\"Find the transcript CSV file robustly\"\"\"\n",
    "    possible_names = [\n",
    "        \"aligned_transcripts.csv\",\n",
    "        \"aligned transcripts.csv\", \n",
    "        \"aligned-transcripts.csv\"\n",
    "    ]\n",
    "    \n",
    "    for name in possible_names:\n",
    "        full_path = os.path.join(os.path.dirname(base_path), name)\n",
    "        if os.path.exists(full_path):\n",
    "            return full_path\n",
    "    \n",
    "    # Search for any file matching pattern\n",
    "    import glob\n",
    "    search_dir = os.path.dirname(base_path)\n",
    "    matches = glob.glob(os.path.join(search_dir, \"*aligned*transcript*.csv\"))\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n",
    "    return base_path  # Return original if nothing found\n",
    "\n",
    "# Load and verify data\n",
    "print(\"Loading transcript data...\")\n",
    "transcript_file = find_transcript_file(TRANSCRIPT_CSV)\n",
    "print(f\"Using transcript file: {transcript_file}\")\n",
    "\n",
    "df = pd.read_csv(transcript_file)\n",
    "print(f\"Loaded {len(df)} transcript entries\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Verify audio files exist\n",
    "print(f\"\\nVerifying audio files in: {AUDIO_DIR}\")\n",
    "missing_files = []\n",
    "existing_files = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    audio_path = os.path.join(AUDIO_DIR, row['wav_filename'])\n",
    "    if os.path.exists(audio_path):\n",
    "        existing_files.append(idx)\n",
    "    else:\n",
    "        missing_files.append((idx, row['wav_filename']))\n",
    "\n",
    "print(f\"Found {len(existing_files)} existing audio files\")\n",
    "print(f\"Missing {len(missing_files)} audio files\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"First few missing files: {missing_files[:5]}\")\n",
    "    # Remove missing files\n",
    "    df = df.iloc[existing_files]\n",
    "    print(f\"Dataset reduced to {len(df)} samples after removing missing files\")\n",
    "\n",
    "# Basic data exploration\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Unique transcripts: {df['transcript'].nunique()}\")\n",
    "print(f\"Average transcript length: {df['transcript'].str.len().mean():.2f}\")\n",
    "print(f\"Sample transcripts:\")\n",
    "for i in range(min(5, len(df))):\n",
    "    print(f\"  {df.iloc[i]['transcript'][:100]}...\")\n",
    "\n",
    "# Audio analysis and preprocessing setup\n",
    "def analyze_audio_lengths(df: pd.DataFrame, audio_dir: str, sample_size: int = 1000) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Analyze audio file lengths to determine max_length for preprocessing\n",
    "    \"\"\"\n",
    "    print(\"Analyzing audio file lengths...\")\n",
    "    \n",
    "    # Sample subset for analysis to speed up\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=SEED)\n",
    "    lengths = []\n",
    "    \n",
    "    for _, row in sample_df.iterrows():\n",
    "        audio_path = os.path.join(audio_dir, row['wav_filename'])\n",
    "        try:\n",
    "            info = torchaudio.info(audio_path)\n",
    "            duration = info.num_frames / info.sample_rate\n",
    "            lengths.append(duration)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {audio_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    lengths = np.array(lengths)\n",
    "    mean_length = np.mean(lengths)\n",
    "    p95_length = np.percentile(lengths, 95)\n",
    "    max_length = np.max(lengths)\n",
    "    \n",
    "    print(f\"Audio length statistics (seconds):\")\n",
    "    print(f\"  Mean: {mean_length:.2f}\")\n",
    "    print(f\"  95th percentile: {p95_length:.2f}\")\n",
    "    print(f\"  Maximum: {max_length:.2f}\")\n",
    "    \n",
    "    return mean_length, p95_length, max_length\n",
    "\n",
    "# Analyze audio\n",
    "mean_len, p95_len, max_len = analyze_audio_lengths(df, AUDIO_DIR)\n",
    "\n",
    "# Use 95th percentile as max length to avoid extreme outliers\n",
    "MAX_AUDIO_SECONDS = min(p95_len * 1.2, 30.0)  # Cap at 30 seconds max\n",
    "print(f\"Using max audio length: {MAX_AUDIO_SECONDS:.2f} seconds\")\n",
    "\n",
    "# Analyze transcript lengths for tokenization\n",
    "print(\"\\nAnalyzing transcript lengths...\")\n",
    "tokenized_lengths = []\n",
    "sample_transcripts = []\n",
    "\n",
    "for transcript in df['transcript'].head(1000):  # Sample for speed\n",
    "    tokens = phoneme_tokenizer.encode(transcript, add_special_tokens=True)\n",
    "    tokenized_lengths.append(len(tokens))\n",
    "    if len(sample_transcripts) < 5:\n",
    "        sample_transcripts.append((transcript, tokens[:10]))  # First 10 tokens\n",
    "\n",
    "tokenized_lengths = np.array(tokenized_lengths)\n",
    "MAX_TOKEN_LENGTH = int(np.percentile(tokenized_lengths, 95) * 1.2)\n",
    "\n",
    "print(f\"Tokenized transcript length statistics:\")\n",
    "print(f\"  Mean: {np.mean(tokenized_lengths):.2f}\")\n",
    "print(f\"  95th percentile: {np.percentile(tokenized_lengths, 95):.2f}\")\n",
    "print(f\"  Using max length: {MAX_TOKEN_LENGTH}\")\n",
    "\n",
    "print(f\"\\nSample tokenizations:\")\n",
    "for transcript, tokens in sample_transcripts[:3]:\n",
    "    print(f\"  Text: {transcript[:50]}...\")\n",
    "    print(f\"  Tokens: {[phoneme_tokenizer.id_to_token[t] for t in tokens]}\")\n",
    "    print(f\"  Decoded: {phoneme_tokenizer.decode(tokens)}\")\n",
    "    print()\n",
    "\n",
    "# Dataset splitting\n",
    "print(\"Splitting dataset...\")\n",
    "\n",
    "# Stratified split to maintain transcript distribution\n",
    "# First split: train vs (val + test)\n",
    "X = df.index.values\n",
    "y = df['transcript'].values  # Use transcript for stratification\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    train_size=TRAIN_SIZE, \n",
    "    random_state=SEED,\n",
    "    stratify=None  # Disable stratification for simplicity with large vocab\n",
    ")\n",
    "\n",
    "# Second split: val vs test from temp\n",
    "val_test_size = len(X_temp)\n",
    "val_ratio = VAL_SIZE / (VAL_SIZE + TEST_SIZE)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    train_size=int(val_test_size * val_ratio),\n",
    "    random_state=SEED,\n",
    "    stratify=None\n",
    ")\n",
    "\n",
    "# Create split dataframes\n",
    "train_df = df.iloc[X_train].reset_index(drop=True)\n",
    "val_df = df.iloc[X_val].reset_index(drop=True) \n",
    "test_df = df.iloc[X_test].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Train: {len(train_df)} samples (target: {TRAIN_SIZE})\")\n",
    "print(f\"  Validation: {len(val_df)} samples (target: {VAL_SIZE})\") \n",
    "print(f\"  Test: {len(test_df)} samples (target: {TEST_SIZE})\")\n",
    "\n",
    "# Check if there are enough samples\n",
    "if len(train_df) < TRAIN_SIZE * 0.8:  # Allow 20% tolerance\n",
    "    print(f\"WARNING: Train dataset ({len(train_df)}) is much smaller than expected ({TRAIN_SIZE})\")\n",
    "    print(\"This will cause fewer steps per epoch than calculated\")\n",
    "\n",
    "# Sanity checks\n",
    "assert abs(len(train_df) - TRAIN_SIZE) <= 5, f\"Train size mismatch: {len(train_df)} vs {TRAIN_SIZE}\"\n",
    "assert abs(len(val_df) - VAL_SIZE) <= 5, f\"Val size mismatch: {len(val_df)} vs {VAL_SIZE}\"\n",
    "assert abs(len(test_df) - TEST_SIZE) <= 5, f\"Test size mismatch: {len(test_df)} vs {TEST_SIZE}\"\n",
    "\n",
    "print(\"✓ Dataset split sanity checks passed\")\n",
    "\n",
    "# Dataset class and data loading\n",
    "class AmharicASRDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for Amharic ASR with phoneme-level targets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dataframe: pd.DataFrame,\n",
    "                 audio_dir: str,\n",
    "                 tokenizer: AmharicPhonemeTokenizer,\n",
    "                 feature_extractor: Wav2Vec2FeatureExtractor,\n",
    "                 max_audio_length: float = 20.0,\n",
    "                 max_token_length: int = 256,\n",
    "                 augment: bool = False):\n",
    "        \n",
    "        self.df = dataframe\n",
    "        self.audio_dir = audio_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_audio_length = max_audio_length\n",
    "        self.max_token_length = max_token_length\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Pre-compute max lengths in samples\n",
    "        self.max_audio_samples = int(max_audio_length * feature_extractor.sampling_rate)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load audio\n",
    "        audio_path = os.path.join(self.audio_dir, row['wav_filename'])\n",
    "        \n",
    "        try:\n",
    "            # Load with torchaudio for consistency\n",
    "            waveform, sample_rate = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Convert to mono if stereo\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            \n",
    "            waveform = waveform.squeeze()\n",
    "            \n",
    "            # Resample if needed\n",
    "            if sample_rate != self.feature_extractor.sampling_rate:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, self.feature_extractor.sampling_rate)\n",
    "                waveform = resampler(waveform)\n",
    "            \n",
    "            # Truncate or pad to max length\n",
    "            if len(waveform) > self.max_audio_samples:\n",
    "                waveform = waveform[:self.max_audio_samples]\n",
    "            elif len(waveform) < self.max_audio_samples:\n",
    "                padding = self.max_audio_samples - len(waveform)\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "            \n",
    "            # Convert to numpy for feature extractor\n",
    "            waveform = waveform.numpy()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio {audio_path}: {e}\")\n",
    "            # Return dummy audio float32\n",
    "            waveform = np.zeros(self.max_audio_samples, dtype=np.float32)\n",
    "\n",
    "        # Process with feature extractor\n",
    "        inputs = self.feature_extractor(\n",
    "            waveform,\n",
    "            sampling_rate=self.feature_extractor.sampling_rate,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize transcript\n",
    "        transcript = row['transcript']\n",
    "        token_ids = self.tokenizer.encode(transcript, add_special_tokens=False)\n",
    "        \n",
    "        # Pad or truncate tokens\n",
    "        if len(token_ids) > self.max_token_length:\n",
    "            token_ids = token_ids[:self.max_token_length]\n",
    "        \n",
    "        return {\n",
    "            'input_values': inputs.input_values.squeeze(),\n",
    "            'labels': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'wav_filename': row['wav_filename'],\n",
    "            'transcript': transcript\n",
    "        }\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AmharicASRDataset(\n",
    "    train_df, AUDIO_DIR, phoneme_tokenizer, feature_extractor, \n",
    "    MAX_AUDIO_SECONDS, MAX_TOKEN_LENGTH, augment=False\n",
    ")\n",
    "\n",
    "val_dataset = AmharicASRDataset(\n",
    "    val_df, AUDIO_DIR, phoneme_tokenizer, feature_extractor,\n",
    "    MAX_AUDIO_SECONDS, MAX_TOKEN_LENGTH, augment=False\n",
    ")\n",
    "\n",
    "test_dataset = AmharicASRDataset(\n",
    "    test_df, AUDIO_DIR, phoneme_tokenizer, feature_extractor,\n",
    "    MAX_AUDIO_SECONDS, MAX_TOKEN_LENGTH, augment=False\n",
    ")\n",
    "\n",
    "print(f\"Created datasets:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\") \n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Test dataset loading\n",
    "print(\"\\nTesting dataset loading...\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Input values shape: {sample['input_values'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"Labels: {sample['labels'][:10].tolist()}\")\n",
    "print(f\"Transcript: {sample['transcript']}\")\n",
    "print(f\"Decoded labels: {phoneme_tokenizer.decode(sample['labels'].tolist())}\")\n",
    "\n",
    "# Data collator for CTC training\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding: \n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    tokenizer: AmharicPhonemeTokenizer\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split into input vs label features\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        # Pad inputs with the feature_extractor (returns tensors)\n",
    "        batch = self.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Correct label padding: pad with -100 so loss ignores padding positions\n",
    "        labels_max_len = max(len(f[\"input_ids\"]) for f in label_features)\n",
    "        labels_batch = -100 * torch.ones((len(label_features), labels_max_len), dtype=torch.long)\n",
    "\n",
    "        for i, label_feature in enumerate(label_features):\n",
    "            labels = label_feature[\"input_ids\"]\n",
    "            if isinstance(labels, list):\n",
    "                labels = torch.tensor(labels, dtype=torch.long)\n",
    "            labels_batch[i, : len(labels)] = labels\n",
    "\n",
    "        batch[\"labels\"] = labels_batch\n",
    "       \n",
    "        return batch\n",
    "\n",
    "# instantiate data collator (replaces previous one)\n",
    "data_collator = DataCollatorCTCWithPadding(\n",
    "    feature_extractor=feature_extractor,\n",
    "    tokenizer=phoneme_tokenizer,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Test data collator\n",
    "print(\"Testing data collator...\")\n",
    "test_batch = [train_dataset[i] for i in range(3)]\n",
    "collated_batch = data_collator(test_batch)\n",
    "print(f\"Collated batch keys: {collated_batch.keys()}\")\n",
    "print(f\"Input values shape: {collated_batch['input_values'].shape}\")\n",
    "print(f\"Labels shape: {collated_batch['labels'].shape}\")\n",
    " \n",
    "# Model setup\n",
    "print(\"Loading pre-trained Wav2Vec2 model...\")\n",
    "\n",
    "# Load pre-trained model\n",
    "# Safe model loading: try the full kw args, fallback to a simpler call if the installed HF is older.\n",
    "try:\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"facebook/wav2vec2-large-xlsr-53\",\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=phoneme_tokenizer.pad_token_id,\n",
    "        vocab_size=phoneme_tokenizer.vocab_size,\n",
    "    )\n",
    "except TypeError as e:\n",
    "    # Older transformers might not accept some kwargs — fall back to default load then patch config.\n",
    "    print(\"Warning: from_pretrained rejected some kwargs, reloading without them:\", e)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
    "\n",
    "# Ensure config matches tokenizer (this is safe even if config is frozen in older versions)\n",
    "try:\n",
    "    model.config.vocab_size = phoneme_tokenizer.vocab_size\n",
    "    model.config.pad_token_id = phoneme_tokenizer.pad_token_id\n",
    "    model.config.ctc_loss_reduction = \"mean\"\n",
    "except Exception:\n",
    "    # Some older configs might be read-only; patching above is best-effort and non-fatal.\n",
    "    pass\n",
    "\n",
    "# Freeze feature extractor (common practice for fine-tuning)\n",
    "model.freeze_feature_extractor()\n",
    "\n",
    "print(f\"Model vocab size: {model.config.vocab_size}\")\n",
    "print(f\"Tokenizer vocab size: {phoneme_tokenizer.vocab_size}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model moved to device: {device}\")\n",
    "\n",
    "# --- Patch model.config so it matches the phoneme tokenizer (safe / non-fatal) ---\n",
    "try:\n",
    "    if \"model\" in globals() and model is not None:\n",
    "        model.config.vocab_size = phoneme_tokenizer.vocab_size\n",
    "        model.config.pad_token_id = phoneme_tokenizer.pad_token_id\n",
    "        # Ensure CTC reduction defined\n",
    "        try:\n",
    "            model.config.ctc_loss_reduction = \"mean\"\n",
    "        except Exception:\n",
    "            # older config objects might be frozen; ignore if not settable\n",
    "            pass\n",
    "        print(\"Patched model.config to match phoneme tokenizer.\")\n",
    "except Exception as e:\n",
    "    print(\"Could not patch model.config (non-fatal):\", e)\n",
    "\n",
    "print(\"Data collator and training arguments are ready.\")\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "# Create processor combining feature extractor and tokenizer  \n",
    "# Note: We use feature_extractor directly since our tokenizer is custom\n",
    "processor = feature_extractor  # Store reference for later use\n",
    "\n",
    "# Metrics computation functions\n",
    "# Load evaluation metrics\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def compute_metrics_ctc(pred, tokenizer, decode_with_lm=False):\n",
    "    \"\"\"\n",
    "    Compute WER and CER for CTC predictions\n",
    "    \"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    \n",
    "    # CTC decode - remove blanks and repeated tokens\n",
    "    def ctc_decode(pred_ids_batch):\n",
    "        decoded_batch = []\n",
    "        blank_id = tokenizer.blank_token_id\n",
    "        \n",
    "        for pred_ids_seq in pred_ids_batch:\n",
    "            # Remove consecutive duplicates and blanks\n",
    "            decoded_seq = []\n",
    "            prev_id = -1\n",
    "            \n",
    "            for token_id in pred_ids_seq:\n",
    "                if token_id != blank_id and token_id != prev_id:\n",
    "                    decoded_seq.append(token_id)\n",
    "                prev_id = token_id\n",
    "            \n",
    "            decoded_batch.append(decoded_seq)\n",
    "        \n",
    "        return decoded_batch\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_preds = ctc_decode(pred_ids)\n",
    "    \n",
    "    # Decode labels (remove -100 tokens)\n",
    "    pred_labels = pred.label_ids\n",
    "    decoded_labels = []\n",
    "    \n",
    "    for label_seq in pred_labels:\n",
    "        # Remove -100 (padding tokens)\n",
    "        label_seq_clean = label_seq[label_seq != -100]\n",
    "        decoded_labels.append(label_seq_clean.tolist())\n",
    "    \n",
    "    # Convert to text for WER/CER computation\n",
    "    pred_texts = [tokenizer.decode(seq) for seq in decoded_preds]\n",
    "    label_texts = [tokenizer.decode(seq) for seq in decoded_labels]\n",
    "    \n",
    "    # Compute metrics\n",
    "    wer = wer_metric.compute(predictions=pred_texts, references=label_texts)\n",
    "    cer = cer_metric.compute(predictions=pred_texts, references=label_texts)\n",
    "    \n",
    "    # Token-level accuracy\n",
    "    correct_tokens = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for pred_seq, label_seq in zip(decoded_preds, decoded_labels):\n",
    "        # Align sequences for token accuracy\n",
    "        min_len = min(len(pred_seq), len(label_seq))\n",
    "        correct_tokens += sum(p == l for p, l in zip(pred_seq[:min_len], label_seq[:min_len]))\n",
    "        total_tokens += max(len(pred_seq), len(label_seq))\n",
    "    \n",
    "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"wer\": wer,\n",
    "        \"cer\": cer, \n",
    "        \"token_accuracy\": token_accuracy\n",
    "    }\n",
    "\n",
    "# AASRTrainer with guaranteed epoch metrics via a callback\n",
    "class EpochMetricsCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Callback that ensures trainer.epoch_metrics is appended at the end of every epoch.\n",
    "    It computes a small subset evaluation on train and val and stores a normalized dict.\n",
    "    \"\"\"\n",
    "    def __init__(self, trainer_ref, train_subset_size: int = 500, val_subset_size: int = 500):\n",
    "        self.trainer_ref = trainer_ref\n",
    "        self.train_subset_size = train_subset_size\n",
    "        self.val_subset_size = val_subset_size\n",
    "        # print header flag\n",
    "        self._printed_header = False\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Only run on main process\n",
    "        if not getattr(state, \"is_local_process_zero\", True):\n",
    "            return\n",
    "    \n",
    "        # Remove the step-based check - let the trainer handle epoch timing\n",
    "        trainer_ref = self.trainer_ref\n",
    "    \n",
    "        try:\n",
    "            # Use state.epoch directly \n",
    "            epoch_idx = int(state.epoch) if hasattr(state, 'epoch') else 1\n",
    "    \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"EPOCH {epoch_idx} EVALUATION\")\n",
    "            print(f\"{'='*50}\")\n",
    "    \n",
    "            # Build small val subset if available\n",
    "            eval_res = {}\n",
    "            try:\n",
    "                if trainer_ref.eval_dataset is not None and len(trainer_ref.eval_dataset) > 0:\n",
    "                    print(\"Evaluating on validation set...\")\n",
    "                    val_n = min(self.val_subset_size, len(trainer_ref.eval_dataset))\n",
    "                    val_eval_ds = Subset(trainer_ref.eval_dataset, list(range(val_n)))\n",
    "                    eval_res = trainer_ref.evaluate(eval_dataset=val_eval_ds, metric_key_prefix=\"eval\")\n",
    "                else:\n",
    "                    eval_res = {}\n",
    "            except Exception as e:\n",
    "                print(\"EpochMetricsCallback: eval failed:\", e)\n",
    "                eval_res = {}\n",
    "    \n",
    "            # Build small train subset evaluation\n",
    "            train_res = {}\n",
    "            try:\n",
    "                if trainer_ref.train_dataset is not None and len(trainer_ref.train_dataset) > 0:\n",
    "                    print(\"Evaluating on training set...\")\n",
    "                    train_n = min(self.train_subset_size, len(trainer_ref.train_dataset))\n",
    "                    train_eval_ds = Subset(trainer_ref.train_dataset, list(range(train_n)))\n",
    "                    train_res = trainer_ref.evaluate(eval_dataset=train_eval_ds, metric_key_prefix=\"train\")\n",
    "                else:\n",
    "                    train_res = {}\n",
    "            except Exception as e:\n",
    "                print(\"EpochMetricsCallback: train-eval failed:\", e)\n",
    "                train_res = {}\n",
    "    \n",
    "            # Normalize keys and pick sensible defaults\n",
    "            def _get(mdict, keys, default=None):\n",
    "                for k in keys:\n",
    "                    if k in mdict:\n",
    "                        return mdict[k]\n",
    "                return default\n",
    "    \n",
    "            epoch_entry = {\n",
    "                \"epoch\": epoch_idx,\n",
    "                \"train_loss\": _get(train_res, [\"train_loss\", \"loss\"], None),\n",
    "                \"train_wer\":  _get(train_res, [\"train_wer\", \"wer\"], None),\n",
    "                \"train_cer\":  _get(train_res, [\"train_cer\", \"cer\"], None),\n",
    "                \"eval_loss\":  _get(eval_res,  [\"eval_loss\", \"loss\"], None),\n",
    "                \"eval_wer\":   _get(eval_res,  [\"eval_wer\", \"wer\"], None),\n",
    "                \"eval_cer\":   _get(eval_res,  [\"eval_cer\", \"cer\"], None),\n",
    "            }\n",
    "    \n",
    "            # Ensure trainer has epoch_metrics list\n",
    "            if not hasattr(trainer_ref, \"epoch_metrics\") or trainer_ref.epoch_metrics is None:\n",
    "                trainer_ref.epoch_metrics = []\n",
    "            trainer_ref.epoch_metrics.append(epoch_entry)\n",
    "    \n",
    "            # Print header once\n",
    "            if not self._printed_header:\n",
    "                print(f\"\\n{'='*110}\")\n",
    "                print(f\"{'Epoch':<6} {'Training Loss':<14} {'Validation Loss':<16} {'Training WER':<13} {'Training CER':<13} {'Validation WER':<15} {'Validation CER':<15}\")\n",
    "                print(f\"{'='*110}\")\n",
    "                self._printed_header = True\n",
    "    \n",
    "            # Pretty print epoch summary\n",
    "            train_loss_val = epoch_entry['train_loss'] or 0\n",
    "            eval_loss_val = epoch_entry['eval_loss'] or 0\n",
    "            train_wer_val = epoch_entry['train_wer'] or 0\n",
    "            train_cer_val = epoch_entry['train_cer'] or 0\n",
    "            eval_wer_val = epoch_entry['eval_wer'] or 0\n",
    "            eval_cer_val = epoch_entry['eval_cer'] or 0\n",
    "            \n",
    "            print(f\"{epoch_idx:<6} {train_loss_val:<14.6f} {eval_loss_val:<16.6f} {train_wer_val:<13.4f} {train_cer_val:<13.4f} {eval_wer_val:<15.4f} {eval_cer_val:<15.4f}\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            # Non-fatal: don't crash training if callback fails\n",
    "            print(\"EpochMetricsCallback: unexpected error:\", e)\n",
    "            import traceback\n",
    "            traceback.print_exc() \n",
    "\n",
    "class ASRTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Trainer subclass that ensures epoch_metrics exist and automatically registers\n",
    "    the EpochMetricsCallback so each epoch appends results to trainer.epoch_metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, phoneme_tokenizer=None, epoch_callback_train_subset=500, epoch_callback_val_subset=500, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.phoneme_tokenizer = phoneme_tokenizer\n",
    "        # ensure attribute exists\n",
    "        self.epoch_metrics = getattr(self, \"epoch_metrics\", [])\n",
    "        # create callback and register it robustly\n",
    "        self._epoch_metrics_cb = EpochMetricsCallback(self, train_subset_size=epoch_callback_train_subset, val_subset_size=epoch_callback_val_subset)\n",
    "        try:\n",
    "            # Trainer has add_callback helper in many versions\n",
    "            if hasattr(self, \"add_callback\"):\n",
    "                self.add_callback(self._epoch_metrics_cb)\n",
    "            else:\n",
    "                # fallback: use callback_handler if present\n",
    "                if hasattr(self, \"callback_handler\") and hasattr(self.callback_handler, \"add_callback\"):\n",
    "                    self.callback_handler.add_callback(self._epoch_metrics_cb)\n",
    "                else:\n",
    "                    # As last resort, append to the callbacks list passed during initialization (if accessible)\n",
    "                    if hasattr(self, \"callbacks\") and isinstance(self.callbacks, list):\n",
    "                        self.callbacks.append(self._epoch_metrics_cb)\n",
    "        except Exception as e:\n",
    "            print(\"AASRTrainer: could not register epoch metrics callback automatically:\", e)\n",
    "\n",
    "# Training setup and arguments\n",
    "print(\"Setting up training arguments (version-robust)...\")\n",
    "\n",
    "# Number of training samples - use actual dataset size\n",
    "num_train_samples = len(train_dataset)\n",
    "\n",
    "# determine world size safely\n",
    "try:\n",
    "    world_size = torch.distributed.get_world_size() if torch.distributed.is_initialized() else 1\n",
    "except Exception:\n",
    "    world_size = 1\n",
    "\n",
    "# compute steps per epoch (ceil to account for remainder) - Multi-GPU aware\n",
    "try:\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        effective_batch_size = BATCH_SIZE * torch.cuda.device_count()\n",
    "        print(f\"Multi-GPU detected: {torch.cuda.device_count()} GPUs\")\n",
    "        print(f\"Effective batch size: {BATCH_SIZE} × {torch.cuda.device_count()} = {effective_batch_size}\")\n",
    "    else:\n",
    "        effective_batch_size = BATCH_SIZE * world_size\n",
    "except:\n",
    "    effective_batch_size = BATCH_SIZE * world_size\n",
    "steps_per_epoch = math.ceil(num_train_samples / effective_batch_size)\n",
    "total_steps = steps_per_epoch * EPOCHS \n",
    "\n",
    "# Add diagnostics\n",
    "print(f\"DATASET DIAGNOSTICS:\")\n",
    "print(f\"Actual train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Expected train dataset size: {TRAIN_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Corrected steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total steps: {total_steps}\")\n",
    " \n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Training samples: {num_train_samples}\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"  Total epochs: {EPOCHS}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "\n",
    "# Check what evaluation parameter name is supported\n",
    "sig = inspect.signature(TrainingArguments)\n",
    "accepted_params = set(sig.parameters.keys())\n",
    "\n",
    "# Handle both old and new parameter names for evaluation strategy\n",
    "eval_strategy_param = None\n",
    "if \"evaluation_strategy\" in accepted_params:\n",
    "    eval_strategy_param = \"evaluation_strategy\"\n",
    "elif \"eval_strategy\" in accepted_params:\n",
    "    eval_strategy_param = \"eval_strategy\"\n",
    "\n",
    "print(f\"Using evaluation strategy parameter: {eval_strategy_param}\")\n",
    "\n",
    "# Build candidate args with correct parameter names\n",
    "candidate_training_args = {\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"group_by_length\": False,\n",
    "    \"per_device_train_batch_size\": BATCH_SIZE,\n",
    "    \"per_device_eval_batch_size\": 16, \n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"save_strategy\": \"epoch\",        \n",
    "    \"num_train_epochs\": EPOCHS,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": torch.cuda.is_available(),\n",
    "    \"logging_steps\": 1,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"weight_decay\": 0.005,\n",
    "    \"warmup_steps\": WARMUP_STEPS,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"load_best_model_at_end\": False,\n",
    "    \"push_to_hub\": False,\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"dataloader_num_workers\": 2,\n",
    "    \"report_to\": [],\n",
    "    \"disable_tqdm\": False,\n",
    "    \"seed\": SEED,\n",
    "    \"dataloader_pin_memory\": False,  # prevent memory issues\n",
    "}\n",
    "\n",
    "# Add evaluation strategy with correct parameter name\n",
    "if eval_strategy_param:\n",
    "    candidate_training_args[eval_strategy_param] = \"no\"  # Disable built-in evaluation\n",
    "\n",
    "# Filter according to installed TrainingArguments signature\n",
    "filtered_args = {k: v for k, v in candidate_training_args.items() if k in accepted_params}\n",
    "\n",
    "print(\"Filtered training arguments:\")\n",
    "for k in sorted(filtered_args.keys()):\n",
    "    print(f\"  - {k}: {filtered_args[k]}\")\n",
    "\n",
    "# Create TrainingArguments using only accepted params\n",
    "training_args = TrainingArguments(**filtered_args)\n",
    "print(\"TrainingArguments created successfully!\")\n",
    "\n",
    "print(\"TrainingArguments created with:\")\n",
    "for k in sorted(filtered_args.keys()):\n",
    "    print(f\"  - {k}: {filtered_args[k]}\")\n",
    "    \n",
    "class StepProgressCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Prints progress at every step with more reliable loss tracking\n",
    "    \"\"\"\n",
    "    def __init__(self, steps_per_epoch: int, total_epochs: int):\n",
    "        self.steps_per_epoch = int(steps_per_epoch)\n",
    "        self.total_epochs = int(total_epochs)\n",
    "        self.current_loss = None\n",
    "        self.step_count = 0\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"Print clean progress at every step\"\"\"\n",
    "        # Skip first step (usually initialization)\n",
    "        if state.global_step == 0:\n",
    "            return\n",
    "    \n",
    "        # Only print from main process\n",
    "        if not getattr(state, \"is_local_process_zero\", True):\n",
    "            return\n",
    "    \n",
    "        # Compute epoch and step \n",
    "        step_in_epoch = ((state.global_step - 1) % self.steps_per_epoch) + 1\n",
    "        epoch = ((state.global_step - 1) // self.steps_per_epoch) + 1\n",
    "    \n",
    "        # Try to get loss from multiple sources\n",
    "        loss_val = None\n",
    "        if hasattr(kwargs, 'logs') and kwargs['logs'] and 'loss' in kwargs['logs']:\n",
    "            loss_val = kwargs['logs']['loss']\n",
    "        elif state.log_history and 'loss' in state.log_history[-1]:\n",
    "            loss_val = state.log_history[-1]['loss']\n",
    "        \n",
    "        # Print clean progress (only the format you want)\n",
    "        if loss_val is not None:\n",
    "            print(f\"Epoch {epoch}/{self.total_epochs}  Step {step_in_epoch}/{self.steps_per_epoch}  Loss: {loss_val:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}/{self.total_epochs}  Step {step_in_epoch}/{self.steps_per_epoch}\")\n",
    "        \n",
    "        # Flush output to ensure immediate display\n",
    "        import sys\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"Capture loss when logged\"\"\"\n",
    "        if logs and 'loss' in logs:\n",
    "            self.current_loss = logs['loss'] \n",
    "\n",
    "# Define compute_metrics function for trainer\n",
    "def compute_metrics_for_trainer(pred):\n",
    "    return compute_metrics_ctc(pred, phoneme_tokenizer)\n",
    "\n",
    "# Initialize trainer and start training\n",
    "print(\"Initializing trainer...\")\n",
    "\n",
    "# Create progress callback\n",
    "progress_cb = StepProgressCallback(steps_per_epoch=steps_per_epoch, total_epochs=EPOCHS)\n",
    "print(f\"Progress callback initialized with {steps_per_epoch} steps per epoch\") \n",
    "\n",
    "trainer = ASRTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics_for_trainer, \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    phoneme_tokenizer=phoneme_tokenizer,\n",
    "    callbacks=[progress_cb] \n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "if checkpoint is not None:\n",
    "    print(f\"Resuming from checkpoint: {checkpoint}\")\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "    # Save the model and tokenizer\n",
    "    trainer.save_model()\n",
    "    phoneme_tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"Model and tokenizer saved to: {OUTPUT_DIR}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Final evaluation on test set\n",
    "print(\"=\"*50)\n",
    "print(\"FINAL EVALUATION ON TEST SET\") \n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_subset_size = min(500, len(test_dataset))\n",
    "test_subset_indices = list(range(test_subset_size))\n",
    "test_eval_dataset = Subset(test_dataset, test_subset_indices)\n",
    "test_results = trainer.evaluate(eval_dataset=test_eval_dataset, metric_key_prefix=\"test\")\n",
    "\n",
    "print(\"Test Results:\")\n",
    "for key, value in test_results.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# CTC Alignment and Co-articulation Helper Functions\n",
    "def ctc_decode_with_alignment(logits, labels, tokenizer, return_alignment=False):\n",
    "    \"\"\"CTC decode with proper alignment tracking using edit distance.\"\"\"\n",
    "    blank_id = tokenizer.blank_token_id\n",
    "    \n",
    "    # Standard CTC decode (remove blanks and consecutive duplicates)\n",
    "    pred_ids = np.argmax(logits, axis=-1)\n",
    "    decoded_tokens = []\n",
    "    prev_id = -1\n",
    "    \n",
    "    for token_id in pred_ids:\n",
    "        if token_id != blank_id and token_id != prev_id:\n",
    "            decoded_tokens.append(int(token_id))\n",
    "        prev_id = token_id\n",
    "    \n",
    "    if not return_alignment:\n",
    "        return decoded_tokens\n",
    "    \n",
    "    # Clean labels (remove -100 padding)\n",
    "    clean_labels = [int(x) for x in labels if x != -100]\n",
    "    \n",
    "    # Compute edit distance alignment\n",
    "    alignment_path = compute_edit_distance_alignment(decoded_tokens, clean_labels)\n",
    "    return decoded_tokens, alignment_path\n",
    "\n",
    "def compute_edit_distance_alignment(pred_tokens, ref_tokens):\n",
    "    \"\"\"Compute optimal alignment between prediction and reference.\"\"\"\n",
    "    # The editdistance package doesn't have eval_ops function, use DP implementation\n",
    "    return dp_alignment(pred_tokens, ref_tokens)\n",
    "\n",
    "def dp_alignment(pred_tokens, ref_tokens):\n",
    "    \"\"\"Fallback DP-based alignment when editdistance is not available.\"\"\"\n",
    "    m, n = len(pred_tokens), len(ref_tokens)\n",
    "    if m == 0 and n == 0:\n",
    "        return []\n",
    "    if m == 0:\n",
    "        return [(None, j, 'deletion') for j in range(n)]\n",
    "    if n == 0:\n",
    "        return [(i, None, 'insertion') for i in range(m)]\n",
    "    \n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Initialize base cases\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Fill DP table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if pred_tokens[i-1] == ref_tokens[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i-1][j],    # deletion\n",
    "                    dp[i][j-1],    # insertion\n",
    "                    dp[i-1][j-1]   # substitution\n",
    "                )\n",
    "    \n",
    "    # Backtrack to get alignment\n",
    "    alignment = []\n",
    "    i, j = m, n\n",
    "    \n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and pred_tokens[i-1] == ref_tokens[j-1]:\n",
    "            alignment.append((i-1, j-1, 'match'))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and j > 0 and dp[i][j] == dp[i-1][j-1] + 1:\n",
    "            alignment.append((i-1, j-1, 'substitution'))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and dp[i][j] == dp[i-1][j] + 1:\n",
    "            alignment.append((i-1, None, 'insertion'))\n",
    "            i -= 1\n",
    "        elif j > 0 and dp[i][j] == dp[i][j-1] + 1:\n",
    "            alignment.append((None, j-1, 'deletion'))\n",
    "            j -= 1\n",
    "    \n",
    "    return list(reversed(alignment))\n",
    "\n",
    "class CoarticulationAnalyzer:\n",
    "    \"\"\"Analyzer for phoneme co-articulation effects in ASR predictions.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_window = 2  # Look at ±2 phonemes around target\n",
    "        \n",
    "    def extract_phoneme_contexts(self, alignments, decoded_sequences, reference_sequences):\n",
    "        \"\"\"Extract phoneme contexts from aligned predictions.\"\"\"\n",
    "        contexts = defaultdict(lambda: {\n",
    "            'left_contexts': [], \n",
    "            'right_contexts': [], \n",
    "            'errors': [],\n",
    "            'total_count': 0\n",
    "        })\n",
    "        \n",
    "        # Process each sequence\n",
    "        for alignment, decoded_seq, ref_seq in zip(alignments, decoded_sequences, reference_sequences):\n",
    "            # Create sequence with alignment info\n",
    "            sequence_data = []\n",
    "            \n",
    "            for pred_idx, ref_idx, operation in alignment:\n",
    "                if ref_idx is not None and ref_idx < len(ref_seq):\n",
    "                    ref_token = ref_seq[ref_idx]\n",
    "                    ref_phoneme = self.tokenizer.id_to_token.get(ref_token, f\"UNK_{ref_token}\")\n",
    "                    \n",
    "                    sequence_data.append({\n",
    "                        'phoneme': ref_phoneme,\n",
    "                        'token_id': ref_token,\n",
    "                        'operation': operation,\n",
    "                        'position': len(sequence_data)\n",
    "                    })\n",
    "            \n",
    "            # Analyze contexts for each phoneme in the sequence\n",
    "            for i, phoneme_data in enumerate(sequence_data):\n",
    "                target_phoneme = phoneme_data['phoneme']\n",
    "                is_error = phoneme_data['operation'] != 'match'\n",
    "                \n",
    "                # Extract left context\n",
    "                left_context = []\n",
    "                for j in range(max(0, i - self.context_window), i):\n",
    "                    left_context.append(sequence_data[j]['phoneme'])\n",
    "                \n",
    "                # Extract right context\n",
    "                right_context = []\n",
    "                for j in range(i + 1, min(len(sequence_data), i + self.context_window + 1)):\n",
    "                    right_context.append(sequence_data[j]['phoneme'])\n",
    "                \n",
    "                # Store context information\n",
    "                contexts[target_phoneme]['left_contexts'].append(tuple(left_context))\n",
    "                contexts[target_phoneme]['right_contexts'].append(tuple(right_context))\n",
    "                contexts[target_phoneme]['errors'].append(is_error)\n",
    "                contexts[target_phoneme]['total_count'] += 1\n",
    "        \n",
    "        return dict(contexts)\n",
    "    \n",
    "    def analyze_coarticulation_effects(self, contexts):\n",
    "        \"\"\"Analyze co-articulation effects on phoneme recognition.\"\"\"\n",
    "        coarticulation_stats = {}\n",
    "        \n",
    "        for target_phoneme, context_data in contexts.items():\n",
    "            if context_data['total_count'] < 10:  # Skip phonemes with too few samples\n",
    "                continue\n",
    "            \n",
    "            total_occurrences = len(context_data['errors'])\n",
    "            total_errors = sum(context_data['errors'])\n",
    "            base_error_rate = total_errors / total_occurrences if total_occurrences > 0 else 0\n",
    "            \n",
    "            # Analyze left context effects\n",
    "            left_context_errors = defaultdict(list)\n",
    "            for left_ctx, is_error in zip(context_data['left_contexts'], context_data['errors']):\n",
    "                left_context_errors[left_ctx].append(is_error)\n",
    "            \n",
    "            # Analyze right context effects\n",
    "            right_context_errors = defaultdict(list)\n",
    "            for right_ctx, is_error in zip(context_data['right_contexts'], context_data['errors']):\n",
    "                right_context_errors[right_ctx].append(is_error)\n",
    "            \n",
    "            # Find problematic contexts\n",
    "            problematic_left_contexts = []\n",
    "            for ctx, errors in left_context_errors.items():\n",
    "                if len(errors) >= 3:  # Minimum sample size\n",
    "                    ctx_error_rate = sum(errors) / len(errors)\n",
    "                    if ctx_error_rate > base_error_rate * 1.2:  # 20% higher than base rate\n",
    "                        problematic_left_contexts.append((ctx, ctx_error_rate, len(errors)))\n",
    "            \n",
    "            problematic_right_contexts = []\n",
    "            for ctx, errors in right_context_errors.items():\n",
    "                if len(errors) >= 3:\n",
    "                    ctx_error_rate = sum(errors) / len(errors)\n",
    "                    if ctx_error_rate > base_error_rate * 1.2:\n",
    "                        problematic_right_contexts.append((ctx, ctx_error_rate, len(errors)))\n",
    "            \n",
    "            coarticulation_stats[target_phoneme] = {\n",
    "                'total_occurrences': total_occurrences,\n",
    "                'base_error_rate': base_error_rate,\n",
    "                'problematic_left_contexts': sorted(problematic_left_contexts, key=lambda x: x[1], reverse=True),\n",
    "                'problematic_right_contexts': sorted(problematic_right_contexts, key=lambda x: x[1], reverse=True)\n",
    "            }\n",
    "        \n",
    "        return coarticulation_stats\n",
    "\n",
    "print(\"Helper functions loaded successfully!\")\n",
    "# Proper CTC Analysis and Co-articulation Effects\n",
    "print(\"=\"*60)\n",
    "print(\"DETAILED ANALYSIS: CTC ALIGNMENT & CO-ARTICULATION\") \n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"Performing proper CTC analysis with alignment...\")\n",
    "\n",
    "# Collect model predictions with proper alignment\n",
    "model.eval()\n",
    "all_alignments = []\n",
    "all_decoded_sequences = []\n",
    "all_reference_sequences = []\n",
    "phoneme_error_counts = defaultdict(lambda: {\"correct\": 0, \"substitutions\": 0, \"insertions\": 0, \"deletions\": 0})\n",
    "\n",
    "# Process test samples\n",
    "num_samples = min(200, len(test_dataset))  # Start with smaller number\n",
    "print(f\"Analyzing {num_samples} test samples...\")\n",
    "\n",
    "processed_count = 0  # Track successful samples\n",
    "with torch.no_grad():\n",
    "    for i in range(num_samples):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"  Processing sample {i}/{num_samples}... (successful: {processed_count})\")\n",
    "        \n",
    "        sample = test_dataset[i]\n",
    "        \n",
    "        # Get model prediction\n",
    "        input_values = sample[\"input_values\"].unsqueeze(0).to(device)\n",
    "        logits = model(input_values).logits[0]  # Remove batch dimension\n",
    "        labels = sample[\"labels\"]\n",
    "        \n",
    "        # CTC decode with alignment\n",
    "        try:\n",
    "            decoded_tokens, alignment = ctc_decode_with_alignment(\n",
    "                logits.cpu().numpy(), labels.numpy(), phoneme_tokenizer, return_alignment=True\n",
    "            )\n",
    "            \n",
    "            # Clean reference tokens (remove padding)\n",
    "            clean_ref_tokens = [int(x) for x in labels.numpy() if x != -100]\n",
    "            \n",
    "            # Only proceed if we have valid data\n",
    "            if decoded_tokens and clean_ref_tokens and alignment:\n",
    "                all_alignments.append(alignment)\n",
    "                all_decoded_sequences.append(decoded_tokens)\n",
    "                all_reference_sequences.append(clean_ref_tokens)\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Count phoneme-level errors with proper alignment\n",
    "                for pred_idx, ref_idx, operation in alignment:\n",
    "                    if ref_idx is not None and ref_idx < len(clean_ref_tokens):\n",
    "                        ref_token = clean_ref_tokens[ref_idx]\n",
    "                        ref_phoneme = phoneme_tokenizer.id_to_token.get(ref_token, f\"UNK_{ref_token}\")\n",
    "                        \n",
    "                        if operation == 'match':\n",
    "                            phoneme_error_counts[ref_phoneme][\"correct\"] += 1\n",
    "                        elif operation == 'substitution':\n",
    "                            phoneme_error_counts[ref_phoneme][\"substitutions\"] += 1\n",
    "                        elif operation == 'deletion':\n",
    "                            phoneme_error_counts[ref_phoneme][\"deletions\"] += 1\n",
    "                    elif pred_idx is not None and operation == 'insertion':\n",
    "                        # Handle insertions if needed\n",
    "                        pass\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing sample {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Successfully processed {processed_count} samples out of {num_samples}\")\n",
    "\n",
    "# Calculate properly aligned error statistics\n",
    "print(\"\\nProperly Aligned Phoneme Error Analysis:\")\n",
    "print(\"=\"*50)\n",
    "for phoneme, errors in list(phoneme_error_counts.items())[:10]:  # Show top 10\n",
    "    total = errors[\"correct\"] + errors[\"substitutions\"] + errors[\"deletions\"]\n",
    "    if total > 0:\n",
    "        error_rate = (errors[\"substitutions\"] + errors[\"deletions\"]) / total\n",
    "        print(f\"{phoneme:>8}: {error_rate:.3f} error rate ({total} total occurrences)\")\n",
    "\n",
    "# ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CO-ARTICULATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize co-articulation analyzer\n",
    "coart_analyzer = CoarticulationAnalyzer(phoneme_tokenizer)\n",
    "\n",
    "# Extract phoneme contexts\n",
    "print(\"Extracting phoneme contexts...\")\n",
    "contexts = coart_analyzer.extract_phoneme_contexts(\n",
    "    all_alignments, all_decoded_sequences, all_reference_sequences\n",
    ")\n",
    "\n",
    "print(f\"Extracted contexts for {len(contexts)} different phonemes\")\n",
    "\n",
    "# Analyze co-articulation effects\n",
    "print(\"Analyzing co-articulation effects...\")\n",
    "coarticulation_stats = coart_analyzer.analyze_coarticulation_effects(contexts)\n",
    "\n",
    "print(f\"\\nCo-articulation Analysis Results:\")\n",
    "print(f\"Analyzed {len(coarticulation_stats)} phonemes with sufficient data\")\n",
    "\n",
    "# Show most context-sensitive phonemes\n",
    "print(f\"\\nTop 10 Most Context-Sensitive Phonemes:\")\n",
    "sensitivity_scores = []\n",
    "for phoneme, stats in coarticulation_stats.items():\n",
    "    total_problematic = len(stats['problematic_left_contexts']) + len(stats['problematic_right_contexts'])\n",
    "    sensitivity_scores.append((phoneme, total_problematic, stats['base_error_rate']))\n",
    "\n",
    "sensitivity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "for i, (phoneme, problematic_count, error_rate) in enumerate(sensitivity_scores[:10]):\n",
    "    print(f\"  {i+1:2d}. {phoneme:>8}: {problematic_count:2d} problematic contexts, {error_rate:.3f} base error rate\")\n",
    "\n",
    "# Show detailed examples for most problematic phonemes\n",
    "print(f\"\\nDetailed Analysis of Most Context-Sensitive Phonemes:\")\n",
    "print(\"=\"*60)\n",
    "for phoneme, problematic_count, error_rate in sensitivity_scores[:3]:  # Top 3\n",
    "    stats = coarticulation_stats[phoneme]\n",
    "    print(f\"\\nPHONEME: {phoneme}\")\n",
    "    print(f\"  Base error rate: {error_rate:.3f}\")\n",
    "    print(f\"  Total occurrences: {stats['total_occurrences']}\")\n",
    "    \n",
    "    if stats['problematic_left_contexts']:\n",
    "        print(f\"  Problematic LEFT contexts:\")\n",
    "        for ctx, ctx_error_rate, count in stats['problematic_left_contexts'][:3]:\n",
    "            print(f\"    {str(ctx):>20} -> {phoneme}: {ctx_error_rate:.3f} error rate ({count} samples)\")\n",
    "    \n",
    "    if stats['problematic_right_contexts']:\n",
    "        print(f\"  Problematic RIGHT contexts:\")\n",
    "        for ctx, ctx_error_rate, count in stats['problematic_right_contexts'][:3]:\n",
    "            print(f\"    {phoneme} -> {str(ctx):>20}: {ctx_error_rate:.3f} error rate ({count} samples)\")\n",
    "\n",
    "# Save co-articulation results with error handling\n",
    "coart_results = []\n",
    "for phoneme, stats in coarticulation_stats.items():\n",
    "    coart_results.append({\n",
    "        'phoneme': phoneme,\n",
    "        'total_occurrences': stats['total_occurrences'],\n",
    "        'base_error_rate': stats['base_error_rate'],\n",
    "        'num_problematic_left_contexts': len(stats['problematic_left_contexts']),\n",
    "        'num_problematic_right_contexts': len(stats['problematic_right_contexts']),\n",
    "        'context_sensitivity_score': len(stats['problematic_left_contexts']) + len(stats['problematic_right_contexts'])\n",
    "    })\n",
    "\n",
    "# Handle empty results gracefully to prevent KeyError\n",
    "if coart_results:\n",
    "    coart_df = pd.DataFrame(coart_results)\n",
    "    coart_df = coart_df.sort_values('context_sensitivity_score', ascending=False)\n",
    "    coart_df.to_csv(f\"{PLOTS_DIR}/coarticulation_analysis.csv\", index=False)\n",
    "    print(f\"Saved co-articulation analysis for {len(coart_df)} phonemes\")\n",
    "else:\n",
    "    # Create empty DataFrame with correct columns to prevent later errors\n",
    "    coart_df = pd.DataFrame(columns=['phoneme', 'total_occurrences', 'base_error_rate', \n",
    "                                   'num_problematic_left_contexts', 'num_problematic_right_contexts',\n",
    "                                   'context_sensitivity_score'])\n",
    "    print(\"Warning: No co-articulation data available - creating empty results file\")\n",
    "    coart_df.to_csv(f\"{PLOTS_DIR}/coarticulation_analysis.csv\", index=False) \n",
    "\n",
    "print(f\"\\nResults saved to:\")\n",
    "print(f\"  - {PLOTS_DIR}/coarticulation_analysis.csv\")\n",
    "\n",
    "# Create simple visualization with error handling\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "if not coart_df.empty and len(coart_df) > 0:\n",
    "    top_10_phonemes = coart_df.head(10)\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(range(len(top_10_phonemes)), top_10_phonemes['context_sensitivity_score'])\n",
    "    plt.xlabel('Phonemes (ranked by context sensitivity)')\n",
    "    plt.ylabel('Context Sensitivity Score')\n",
    "    plt.title('Top 10 Most Context-Sensitive Phonemes')\n",
    "    plt.xticks(range(len(top_10_phonemes)), top_10_phonemes['phoneme'], rotation=45)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(top_10_phonemes['base_error_rate'], top_10_phonemes['context_sensitivity_score'])\n",
    "    plt.xlabel('Base Error Rate')\n",
    "    plt.ylabel('Context Sensitivity Score')\n",
    "    plt.title('Error Rate vs Context Sensitivity')\n",
    "\n",
    "    # Add phoneme labels to points\n",
    "    for i, row in top_10_phonemes.iterrows():\n",
    "        plt.annotate(row['phoneme'], \n",
    "                    (row['base_error_rate'], row['context_sensitivity_score']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "else:\n",
    "    # Show empty plot message\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.text(0.5, 0.5, 'No data available\\nAll samples failed processing', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes, fontsize=14)\n",
    "    plt.title('No Co-articulation Data Available')\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.text(0.5, 0.5, 'Check alignment function\\nand sample processing', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes, fontsize=14)\n",
    "    plt.title('Troubleshooting Required')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PLOTS_DIR}/coarticulation_visualization.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show() \n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Key findings:\")\n",
    "print(f\"1. Most context-sensitive phoneme: {sensitivity_scores[0][0] if sensitivity_scores else 'N/A'}\")\n",
    "print(f\"2. Average context sensitivity: {np.mean([x[1] for x in sensitivity_scores]) if sensitivity_scores else 0:.1f}\")\n",
    "print(f\"3. Phonemes with co-articulation effects: {len([x for x in sensitivity_scores if x[1] > 0])}\")\n",
    "\n",
    "# Plotting and visualization \n",
    "print(\"Creating plots and visualizations...\")\n",
    "\n",
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Training Progress - Train vs Validation', fontsize=16)\n",
    "\n",
    "# Extract metrics from trainer history\n",
    "epochs = []\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "train_wers = []\n",
    "eval_wers = []\n",
    "train_cers = []\n",
    "eval_cers = []\n",
    "\n",
    "if hasattr(trainer, \"epoch_metrics\") and trainer.epoch_metrics:\n",
    "    for metrics in trainer.epoch_metrics:\n",
    "        epochs.append(metrics[\"epoch\"])\n",
    "        # Training metrics\n",
    "        train_losses.append(metrics.get(\"train_loss\"))\n",
    "        train_wers.append(metrics.get(\"train_wer\"))\n",
    "        train_cers.append(metrics.get(\"train_cer\"))\n",
    "        # Validation metrics\n",
    "        eval_losses.append(metrics.get(\"eval_loss\"))\n",
    "        eval_wers.append(metrics.get(\"eval_wer\"))\n",
    "        eval_cers.append(metrics.get(\"eval_cer\"))\n",
    "    \n",
    "    print(f\"Plotting metrics for {len(epochs)} epochs...\")\n",
    "    \n",
    "    # Plot 1: Loss comparison\n",
    "    axes[0, 0].plot(epochs, eval_losses, 'b-', label='Validation Loss', marker='o', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, train_losses, 'r-', label='Training Loss', marker='s', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Plot 2: WER comparison\n",
    "    axes[0, 1].plot(epochs, eval_wers, 'g-', label='Validation WER', marker='o', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, train_wers, 'orange', label='Training WER', marker='s', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('WER')\n",
    "    axes[0, 1].set_title('Word Error Rate')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Plot 3: CER comparison  \n",
    "    axes[1, 0].plot(epochs, eval_cers, 'm-', label='Validation CER', marker='o', linewidth=2)\n",
    "    axes[1, 0].plot(epochs, train_cers, 'purple', label='Training CER', marker='s', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('CER')\n",
    "    axes[1, 0].set_title('Character Error Rate')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Plot 4: Combined Loss and WER\n",
    "    axes[1, 1].plot(epochs, train_losses, 'r-', label='Training Loss', marker='s', linewidth=2)\n",
    "    axes[1, 1].plot(epochs, eval_losses, 'b-', label='Validation Loss', marker='o', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Loss Comparison')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "else:\n",
    "    print(\"WARNING: No epoch metrics found. Check ASRTrainer.epoch_metrics\")\n",
    "    # Create empty plots\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[i, j].text(0.5, 0.5, 'No Data Available', \n",
    "                          horizontalalignment='center', verticalalignment='center',\n",
    "                          transform=axes[i, j].transAxes, fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PLOTS_DIR}/training_progress.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Phoneme-level analysis plots\n",
    "print(\"Creating phoneme-level analysis...\")\n",
    "\n",
    "# Per-phoneme error analysis\n",
    "phoneme_names = []\n",
    "phoneme_error_rates = []\n",
    "phoneme_counts = []\n",
    "\n",
    "for phoneme, errors in phoneme_error_counts.items():\n",
    "    total_occurrences = errors[\"correct\"] + errors[\"substitutions\"] + errors[\"deletions\"]\n",
    "    if total_occurrences > 5:  # Only include phonemes with sufficient occurrences\n",
    "        error_rate = (errors[\"substitutions\"] + errors[\"deletions\"]) / total_occurrences\n",
    "        phoneme_names.append(phoneme)\n",
    "        phoneme_error_rates.append(error_rate)\n",
    "        phoneme_counts.append(total_occurrences)\n",
    "\n",
    "# Sort by error rate\n",
    "sorted_indices = np.argsort(phoneme_error_rates)[::-1]  # Descending order\n",
    "sorted_phonemes = [phoneme_names[i] for i in sorted_indices]\n",
    "sorted_error_rates = [phoneme_error_rates[i] for i in sorted_indices]\n",
    "sorted_counts = [phoneme_counts[i] for i in sorted_indices]\n",
    "\n",
    "# Plot per-phoneme error rates\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(range(len(sorted_phonemes)), sorted_error_rates, \n",
    "               color=plt.cm.viridis(np.linspace(0, 1, len(sorted_phonemes))))\n",
    "plt.xlabel('Phonemes')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Per-Phoneme Error Rates (Sorted by Error Rate)')\n",
    "plt.xticks(range(len(sorted_phonemes)), sorted_phonemes, rotation=45, ha='right')\n",
    "\n",
    "# Add count annotations on bars\n",
    "for i, (bar, count) in enumerate(zip(bars, sorted_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PLOTS_DIR}/per_phoneme_errors.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create confusion matrix for top phonemes\n",
    "top_n_phonemes = 20\n",
    "top_phoneme_ids = []\n",
    "top_phoneme_names = []\n",
    "\n",
    "# Get most frequent phonemes\n",
    "phoneme_freq = {}\n",
    "for ref_tokens in all_reference_sequences:\n",
    "    for token_id in ref_tokens:\n",
    "        if token_id < phoneme_tokenizer.vocab_size:\n",
    "            phoneme_freq[token_id] = phoneme_freq.get(token_id, 0) + 1\n",
    "\n",
    "# Sort by frequency and take top N\n",
    "sorted_phonemes_by_freq = sorted(phoneme_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "top_phonemes = sorted_phonemes_by_freq[:top_n_phonemes]\n",
    "\n",
    "for token_id, freq in top_phonemes:\n",
    "    top_phoneme_ids.append(token_id)\n",
    "    top_phoneme_names.append(phoneme_tokenizer.id_to_token.get(token_id, f\"UNK_{token_id}\"))\n",
    "\n",
    "# Create simplified confusion matrix (this is approximate)\n",
    "conf_matrix = np.zeros((len(top_phoneme_ids), len(top_phoneme_ids)))\n",
    "\n",
    "# Create sample predictions if they don't exist\n",
    "if 'all_predictions' not in globals() or 'all_references' not in globals():\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    for i in range(min(1000, len(all_decoded_sequences), len(all_reference_sequences))):\n",
    "        pred_text = phoneme_tokenizer.decode(all_decoded_sequences[i])\n",
    "        ref_text = phoneme_tokenizer.decode(all_reference_sequences[i])\n",
    "        all_predictions.append(pred_text)\n",
    "        all_references.append(ref_text)\n",
    "\n",
    "# Fill confusion matrix (simplified approach)\n",
    "for pred_text, ref_text in zip(all_predictions[:1000], all_references[:1000]):  # Sample for speed\n",
    "    pred_tokens = phoneme_tokenizer.encode(pred_text, add_special_tokens=False)\n",
    "    ref_tokens = phoneme_tokenizer.encode(ref_text, add_special_tokens=False)\n",
    "    \n",
    "    for p_id, r_id in zip(pred_tokens, ref_tokens):\n",
    "        if p_id in top_phoneme_ids and r_id in top_phoneme_ids:\n",
    "            p_idx = top_phoneme_ids.index(p_id)\n",
    "            r_idx = top_phoneme_ids.index(r_id)\n",
    "            conf_matrix[r_idx, p_idx] += 1\n",
    "\n",
    "# Normalize confusion matrix\n",
    "conf_matrix_norm = conf_matrix / (conf_matrix.sum(axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix_norm, \n",
    "            xticklabels=top_phoneme_names, \n",
    "            yticklabels=top_phoneme_names,\n",
    "            annot=False, \n",
    "            cmap='Blues', \n",
    "            square=True)\n",
    "plt.xlabel('Predicted Phonemes')\n",
    "plt.ylabel('Reference Phonemes')\n",
    "plt.title(f'Phoneme Confusion Matrix (Top {top_n_phonemes} Phonemes)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{PLOTS_DIR}/phoneme_confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save results and create summary files\n",
    "print(\"Saving results and creating summary files...\")\n",
    "\n",
    "# Save epoch-by-epoch metrics\n",
    "metrics_data = {\n",
    "    \"model_config\": {\n",
    "        \"model_name\": \"facebook/wav2vec2-large-xlsr-53\",\n",
    "        \"vocab_size\": phoneme_tokenizer.vocab_size,\n",
    "        \"max_audio_length\": MAX_AUDIO_SECONDS,\n",
    "        \"max_token_length\": MAX_TOKEN_LENGTH,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS\n",
    "    },\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"val_size\": len(val_dataset),\n",
    "        \"test_size\": len(test_dataset),\n",
    "        \"audio_dir\": AUDIO_DIR,\n",
    "        \"transcript_file\": transcript_file\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics to JSON\n",
    "with open(f\"{OUTPUT_DIR}/metrics.json\", 'w') as f:\n",
    "    json.dump(metrics_data, f, indent=2)\n",
    "\n",
    "# Save per-phoneme error analysis\n",
    "phoneme_error_df = pd.DataFrame([\n",
    "    {\n",
    "        \"phoneme\": phoneme,\n",
    "        \"correct\": errors[\"correct\"],\n",
    "        \"substitutions\": errors[\"substitutions\"],\n",
    "        \"insertions\": errors[\"insertions\"],\n",
    "        \"deletions\": errors[\"deletions\"],\n",
    "        \"total_occurrences\": errors[\"correct\"] + errors[\"substitutions\"] + errors[\"deletions\"],\n",
    "        \"error_rate\": (errors[\"substitutions\"] + errors[\"deletions\"]) / max(1, errors[\"correct\"] + errors[\"substitutions\"] + errors[\"deletions\"])\n",
    "    }\n",
    "    for phoneme, errors in phoneme_error_counts.items()  \n",
    "])\n",
    "\n",
    "phoneme_error_df = phoneme_error_df.sort_values(\"error_rate\", ascending=False)\n",
    "phoneme_error_df.to_csv(f\"{PLOTS_DIR}/per_phoneme_errors.csv\", index=False)\n",
    "\n",
    "# Save sample predictions (build sample_predictions_data if missing) — save only 10 examples\n",
    "if 'sample_predictions_data' not in globals():\n",
    "    sample_predictions_data = []\n",
    "    N = min(10, len(all_predictions))   # <-- save only 10\n",
    "    for i in range(N):\n",
    "        try:\n",
    "            wav_name = test_dataset[i].get(\"wav_filename\", f\"sample_{i}\")\n",
    "        except Exception:\n",
    "            wav_name = f\"sample_{i}\"\n",
    "        sample_predictions_data.append({\n",
    "            \"index\": i,\n",
    "            \"wav_filename\": wav_name,\n",
    "            \"prediction\": all_predictions[i],\n",
    "            \"reference\": all_references[i]\n",
    "        })\n",
    "\n",
    "# write dataframe to disk\n",
    "sample_pred_df = pd.DataFrame(sample_predictions_data)\n",
    "sample_pred_df.to_csv(os.path.join(OUTPUT_DIR, \"sample_predictions.csv\"), index=False)\n",
    "print(f\"Saved {len(sample_pred_df)} sample predictions to {OUTPUT_DIR}/sample_predictions.csv\") \n",
    "\n",
    "# Create sanity checks summary\n",
    "sanity_checks = f\"\"\"\n",
    "SANITY CHECKS SUMMARY\n",
    "=====================\n",
    "\n",
    "Dataset Sizes:\n",
    "- Train: {len(train_dataset)} samples (target: {TRAIN_SIZE})\n",
    "- Validation: {len(val_dataset)} samples (target: {VAL_SIZE})\n",
    "- Test: {len(test_dataset)} samples (target: {TEST_SIZE})\n",
    "- Total: {len(train_dataset) + len(val_dataset) + len(test_dataset)}\n",
    "\n",
    "Training Configuration:\n",
    "- Batch size: {BATCH_SIZE}\n",
    "- Steps per epoch: {steps_per_epoch} (target: ~67)\n",
    "- Total epochs: {EPOCHS}\n",
    "- Max audio length: {MAX_AUDIO_SECONDS:.2f} seconds\n",
    "- Max token length: {MAX_TOKEN_LENGTH}\n",
    "\n",
    "Model Configuration:\n",
    "- Vocab size: {phoneme_tokenizer.vocab_size}\n",
    "- Pad token ID: {phoneme_tokenizer.pad_token_id}\n",
    "- Blank token ID: {phoneme_tokenizer.blank_token_id}\n",
    "- UNK token ID: {phoneme_tokenizer.unk_token_id}\n",
    "\n",
    "Files Created:\n",
    "- Model: {OUTPUT_DIR}/\n",
    "- Plots: {PLOTS_DIR}/\n",
    "- Metrics: {OUTPUT_DIR}/metrics.json\n",
    "- Phoneme errors: {PLOTS_DIR}/per_phoneme_errors.csv\n",
    "- Sample predictions: {OUTPUT_DIR}/sample_predictions.csv \n",
    "\"\"\"\n",
    "with open(f\"{OUTPUT_DIR}/sanity_checks.txt\", 'w') as f:\n",
    "    f.write(sanity_checks)\n",
    "\n",
    "print(\"All results saved successfully!\")\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Plots saved to: {PLOTS_DIR}\")\n",
    "print(f\"Summary saved to: {OUTPUT_DIR}/sanity_checks.txt\")    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8291195,
     "sourceId": 13089927,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
